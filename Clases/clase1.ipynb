{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesión 1 Analiss de texto\n",
    "## Introduccion\n",
    "\n",
    "### definicion de análisis de texto\n",
    "\n",
    "El analis de texto es una rama de la ciencia de datos que se centra en la extraccion util y sisgnificativa a pártir de datos de texto no estructurados, es decir no presentan una estructura de diseño y suelen ser datos cualitativos, conjuntos de datos apilados.\n",
    "\n",
    "### Importancia del analisis de Texto\n",
    "\n",
    "Permite tener conocimiento apartir de grandes volumenes de texto, como redes solciales, libros, blogs, paginas web, documentos empresariales, correos electronicos, permitiendo la automatizacion de tareas como son la clasificacion de documentos, el analisis de sentimiento y la extraccion de informacion, como datos estadisticos los cules pueden verse como patrones de escritura o personalizacion de estilos de redaccion.\n",
    "\n",
    "### Aplicacion del Analisis de Texto \n",
    "\n",
    "#### Aplicacones principales\n",
    "    \n",
    "-   Clasificacion de documentos.\n",
    "-   Analisis de sentimiento.\n",
    "-   Extraccion de informacion.\n",
    "-   Generacioin de resumenes automaticos.\n",
    "-   Traduiccion automatica.\n",
    "-   Chatbots y asistentes virtuales.\n",
    "\n",
    "#### Ejemplos de casos de uso\n",
    "\n",
    "1.  Analisis de opiniones en redes sociales para entender la percepcion del cliente.\n",
    "\n",
    "2.  Alanisis de discursos politicos: contraposicioin de discursos busqueda de congruencia y contradicciones en discursos.\n",
    "\n",
    "3.  clasificaciopn de correos electronicos cono spam o no spam.\n",
    "\n",
    "4.  extraccion de entidades (nombres, lugares, fechas) de documentos legales.\n",
    "\n",
    "5.  Análisis de textos periodísticos: Se emplea una técnica conocida como las cinco “W” del periodismo (Who,What, When, Where, Why) para desglosar noticias y artículos. Se evalúa la estructura, el tono, las fuentes y la veracidad de la información.\n",
    "\n",
    "6.  Análisis de textos literarios: Se estudian obras literarias como novelas, poemas o cuentos. Se analizan elementos como la trama, los personajes, el estilo del autor y los temas subyacentes.\n",
    "\n",
    "7.  Análisis de textos históricos: Se investigan documentos antiguos, cartas, diarios y registros históricos. Se busca comprender el contexto, las intenciones del autor y las implicaciones históricas.\n",
    "\n",
    "8.  Análisis de textos científicos: Se examinan artículos académicos, investigaciones y publicaciones científicas. Se evalúa la metodología, los resultados y las conclusiones.\n",
    "\n",
    "9.  Análisis de textos argumentativos: Se descompone un texto en sus partes constituyentes: introducción, desarrollo y conclusión. Se identifican las afirmaciones, las evidencias y la lógica utilizada por el autor.\n",
    "\n",
    "10. Interpretación de figuras retóricas: Se analizan metáforas, hipérboles, ironías y otras figuras literarias presentes en el texto.\n",
    "    \n",
    "11. Interpretación de datos estadísticos: Se extraen conclusiones a partir de datos numéricos presentados en el texto.\n",
    "\n",
    "### Componentes del analisis de Texto\n",
    "\n",
    "#### Preprocesamiento de Texto\n",
    "\n",
    "-   Limpieza de datos: elikminacion de signos de puntuación, números, palabras vacías (stopwords).\n",
    "    \n",
    "-   Tokenizacion: dividir el texto en unidades mas pequeñas (tokens).\n",
    "\n",
    "#### Representccion de Texto\n",
    "\n",
    "\n",
    "-   Tablas y Gráficos:\n",
    "    1. Tablas: Organizan datos en filas y columnas. Pueden mostrar frecuencias, categorías o resúmenes estadísticos.\n",
    "    2.  Gráficos: Representan visualmente patrones y tendencias. Ejemplos incluyen gráficos de barras, líneas, pastel y dispersión.\n",
    "\n",
    "-   Word Clouds (Nubes de palabras): Visualizan las palabras más frecuentes en un texto. El tamaño de cada palabra refleja su importancia.\n",
    "\n",
    "-   Histogramas de Frecuencia: Muestran la distribución de palabras según su frecuencia. Útiles para analizar vocabulario.\n",
    "\n",
    "-   Redes Semánticas: Representan relaciones entre palabras o conceptos. Pueden ser útiles para explorar temas o conexiones.\n",
    "\n",
    "-   Análisis de Sentimiento: Asigna polaridad (positiva, negativa o neutra) a fragmentos de texto. Útil para evaluar opiniones.\n",
    "\n",
    "-   Visualización de Entidades Nominadas: Muestra nombres de personas, lugares, organizaciones, etc., identificados en el texto.\n",
    "\n",
    "-   Mapas de Calor (Heatmaps): Resaltan áreas de interés en un texto. Por ejemplo, las palabras más mencionadas en diferentes secciones.\n",
    "\n",
    "-   Análisis de Coocurrencia: Examina qué palabras aparecen juntas con mayor frecuencia. Puede ayudar a identificar asociaciones.\n",
    "\n",
    "-   Representación de Secuencias: Visualiza la estructura de oraciones o párrafos. Útil para análisis gramatical.\n",
    "\n",
    "-   Resúmenes Textuales: Sintetizan los hallazgos clave en un formato conciso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalacion de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gutenbergpy\n",
      "  Downloading gutenbergpy-0.3.5-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting future>=0.15.2 (from gutenbergpy)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting httpsproxy-urllib2 (from gutenbergpy)\n",
      "  Downloading httpsproxy_urllib2-1.0.tar.gz (28 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting lxml>=3.2.0 (from gutenbergpy)\n",
      "  Downloading lxml-5.2.1-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting pymongo (from gutenbergpy)\n",
      "  Downloading pymongo-4.7.2-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Collecting setuptools>=18.5 (from gutenbergpy)\n",
      "  Using cached setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting chardet (from gutenbergpy)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo->gutenbergpy)\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading gutenbergpy-0.3.5-py3-none-any.whl (22 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "   ---------------------------------------- 0.0/491.3 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 184.3/491.3 kB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 491.3/491.3 kB 6.2 MB/s eta 0:00:00\n",
      "Downloading lxml-5.2.1-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.8/3.8 MB 17.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.9/3.8 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.1/3.8 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.2/3.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 2.2/3.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.4/3.8 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.5/3.8 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.9/3.8 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.1/3.8 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 7.6 MB/s eta 0:00:00\n",
      "Using cached setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 199.4/199.4 kB 11.8 MB/s eta 0:00:00\n",
      "Downloading pymongo-4.7.2-cp312-cp312-win_amd64.whl (485 kB)\n",
      "   ---------------------------------------- 0.0/485.1 kB ? eta -:--:--\n",
      "   --------------------------------------  481.3/485.1 kB 15.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 485.1/485.1 kB 10.1 MB/s eta 0:00:00\n",
      "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "   ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 307.7/307.7 kB 9.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: httpsproxy-urllib2\n",
      "  Building wheel for httpsproxy-urllib2 (pyproject.toml): started\n",
      "  Building wheel for httpsproxy-urllib2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for httpsproxy-urllib2: filename=httpsproxy_urllib2-1.0-py3-none-any.whl size=29258 sha256=f26b6b2681d83288fe9bdd1df6f6f1597837b5f1100a729ea6f8b22f8c2d1ec2\n",
      "  Stored in directory: c:\\users\\juanj\\appdata\\local\\pip\\cache\\wheels\\c6\\c5\\98\\dd27835c8319362f1cef163409f43df38d54f084500fad428c\n",
      "Successfully built httpsproxy-urllib2\n",
      "Installing collected packages: httpsproxy-urllib2, setuptools, lxml, future, dnspython, chardet, pymongo, gutenbergpy\n",
      "Successfully installed chardet-5.2.0 dnspython-2.6.1 future-1.0.0 gutenbergpy-0.3.5 httpsproxy-urllib2-1.0 lxml-5.2.1 pymongo-4.7.2 setuptools-69.5.1\n"
     ]
    }
   ],
   "source": [
    "#https://git.com/raduangelescu/gutenbergpy\n",
    "!pip install gutenbergpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\juanj\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.5.10-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/42.0 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 511.8 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\juanj\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.5 MB 18.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 13.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.2/1.5 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.4/1.5 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading regex-2024.5.10-cp312-cp312-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 268.5/268.5 kB 17.2 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2024.5.10 tqdm-4.66.4\n"
     ]
    }
   ],
   "source": [
    "# The Natural Language Toolkit (NLTK) is a Python package for natural\n",
    "# language processing. NLTK requires Python 3.7, 3.8, 3.9, 3.10 or 3.11.\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtencion de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la libreria de gutenberg para extraer texto libre directo de la pagina web donde se encuentran alojados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gutenbergpy.textget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una foncion apartir de la cual llamaremos al libro o texto, esta funcion nos descarga dos veces el mismo libro, el primero contiene encabezados y el segundo no incluye los encabezados o headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a crear una funcion para llamar al libro empleando el id que tiene asignado\n",
    "def obtener_libro(id = 2701):\n",
    "    raw_book = gutenbergpy.textget.get_text_by_id(id) #se descarga el libro con todo y heders\n",
    "    clean_book = gutenbergpy.textget.strip_headers(raw_book) # al completo le eliminaremos los heders\n",
    "    return clean_book, raw_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado de cadena de texto\n",
    "\n",
    "Identificamos el libro descargado con y sin headers\n",
    "\n",
    "-   libro_limpio = sin headers\n",
    "-   libro = con headeers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "libro_limpio, libro = obtener_libro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'he Project Gutenberg eBook of Moby-Dick; or The Whale, by Herman Melville\\r\\n\\r\\nThis eBook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this eBook or online at\\r\\nwww.gutenberg.org. If you are not located in the United States, you\\r\\nwill have to check the laws of the country where you are located before\\r\\nusing this eBook.\\r\\n\\r\\nTitle: Moby-Dick; or The Whale\\r\\n\\r\\nAuthor: Herman Melville\\r\\n\\r\\nRelease Date: June, 2001 [eBook #2701]\\r\\n[Most recently updated: August 18, 2021]\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\nProduced by: Daniel Lazarus, Jonesey, and David Widger\\r\\n\\r\\n*** START OF THE PROJECT GUTENBERG EBOOK MOBY-DICK; OR THE WHALE ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nMOBY-DICK;\\r\\n\\r\\nor, THE WHALE.\\r\\n\\r\\nBy Herman Melville\\r\\n\\r\\n\\r\\n\\r\\nCONTENTS\\r\\n\\r\\nETYMOLOGY.\\r\\n\\r\\nEXTRACTS (Supplied by a Sub-Sub-Librarian).\\r\\n\\r\\nCHAPTER 1. Loomings.\\r\\n\\r\\nCHAPTER 2. The Carpet-Bag.\\r\\n\\r\\nCHAPTER 3. The Spouter-Inn.\\r\\n\\r\\nCHAPTER 4. The Counterpane.\\r\\n\\r\\nCHAPTER 5. Breakfast.\\r\\n\\r\\nCHAPTER 6. The Street.\\r\\n\\r\\nCHAPTER 7. The Chapel.\\r\\n\\r\\nCHAPTER 8. The Pulpit.\\r\\n\\r\\nCHAPTER 9. The Sermon.\\r\\n\\r\\nCHAPTER 10. A Bosom Friend.\\r\\n\\r\\nCHAPTER 11. Nightgown.\\r\\n\\r\\nCHAPTER 12. Biographical.\\r\\n\\r\\nCHAPTER 13. Wheelbarrow.\\r\\n\\r\\nCHAPTER 14. Nantucket.\\r\\n\\r\\nCHAPTER 15. Chowder.\\r\\n\\r\\nCHAPTER 16. The Ship.\\r\\n\\r\\nCHAPTER 17. The Ramadan.\\r\\n\\r\\nCHAPTER 18. His Mark.\\r\\n\\r\\nCHAPTER 19. The Prophet.\\r\\n\\r\\nCHAPTER 20. All Astir.\\r\\n\\r\\nCHAPTER 21. Going Aboard.\\r\\n\\r\\nCHAPTER 22. Merry Christmas.\\r\\n\\r\\nCHAPTER 23. The Lee Shore.\\r\\n\\r\\nCHAPTER 24. The Advocate.\\r\\n\\r\\nCHAPTER 25. Postscript.\\r\\n\\r\\nCHAPTER 26. Knights and Squires.\\r\\n\\r\\nCHAPTER 27. Knights and Squires.\\r\\n\\r\\nCHAPTER 28. Ahab.\\r\\n\\r\\nCHAPTER 29. Enter Ahab; to Him, Stubb.\\r\\n\\r\\nCHAPTER 30. The Pipe.\\r\\n\\r\\nCHAPTER 31. Queen Mab.\\r\\n\\r\\nCHAPTER 32. Cetology.\\r\\n\\r\\nCHAPTER 33. The Specksnyder.\\r\\n\\r\\nCHAPTER 34. The Cabin-Table.\\r\\n\\r\\nCHAPTER 35. The Mast-Head.\\r\\n\\r\\nCHAPTER 36.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libro[1:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\r\\n\\r\\n\\r\\n\\r\\nMOBY-DICK;\\r\\n\\r\\nor, THE WHALE.\\r\\n\\r\\nBy Herman Melville\\r\\n\\r\\n\\r\\n\\r\\nCONTENTS\\r\\n\\r\\nETYMOLOGY.\\r\\n\\r\\nEXTRACTS (Supplied by a Sub-Sub-Librarian).\\r\\n\\r\\nCHAPTER 1. Loomings.\\r\\n\\r\\nCHAPTER 2. The Carpet-Bag.\\r\\n\\r\\nCHAPTER 3. The Spouter-Inn.\\r\\n\\r\\nCHAPTER 4. The Counterpane.\\r\\n\\r\\nCHAPTER 5. Breakfast.\\r\\n\\r\\nCHAPTER 6. The Street.\\r\\n\\r\\nCHAPTER 7. The Chapel.\\r\\n\\r\\nCHAPTER 8. The Pulpit.\\r\\n\\r\\nCHAPTER 9. The Sermon.\\r\\n\\r\\nCHAPTER 10. A Bosom Friend.\\r\\n\\r\\nCHAPTER 11. Nightgown.\\r\\n\\r\\nCHAPTER 12. Biographical.\\r\\n\\r\\nCHAPTER 13. Wheelbarrow.\\r\\n\\r\\nCHAPTER 14. Nantucket.\\r\\n\\r\\nCHAPTER 15. Chowder.\\r\\n\\r\\nCHAPTER 16. The Ship.\\r\\n\\r\\nCHAPTER 17. The Ramadan.\\r\\n\\r\\nCHAPTER 18. His Mark.\\r\\n\\r\\nCHAPTER 19. The Prophet.\\r\\n\\r\\nCHAPTER 20. All Astir.\\r\\n\\r\\nCHAPTER 21. Going Aboard.\\r\\n\\r\\nCHAPTER 22. Merry Christmas.\\r\\n\\r\\nCHAPTER 23. The Lee Shore.\\r\\n\\r\\nCHAPTER 24. The Advocate.\\r\\n\\r\\nCHAPTER 25. Postscript.\\r\\n\\r\\nCHAPTER 26. Knights and Squires.\\r\\n\\r\\nCHAPTER 27. Knights and Squires.\\r\\n\\r\\nCHAPTER 28. Ahab.\\r\\n\\r\\nCHAPTER 29. Enter Ahab; to Him, Stubb.\\r\\n\\r\\nCHAPTER 30. The Pipe.\\r\\n\\r\\nCHAPTER 31. Q'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libro_limpio[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder trabajar con el contenido (las palabras) es importante tener la informacion en un formato que facilite la labor por lo tanto tenemos que identificar el tipo dato es el que esta representado el texto es importante considerar el tipo de archivo para poder tener un formato para poder analizar el contenido, pasaremos de bytes a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\r\\n\\r\\n\\r\\n\\r\\nMOBY-DICK;\\r\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(type(libro_limpio))\n",
    "str_libro = libro_limpio.decode()\n",
    "print(type(str_libro))\n",
    "str_libro[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\r',\n",
       " '\\r',\n",
       " '\\r',\n",
       " '\\r',\n",
       " 'MOBY-DICK;\\r',\n",
       " '\\r',\n",
       " 'or, THE WHALE.\\r',\n",
       " '\\r',\n",
       " 'By Herman Melville\\r',\n",
       " '\\r',\n",
       " '\\r',\n",
       " '\\r',\n",
       " 'CONTENTS\\r',\n",
       " '\\r',\n",
       " 'ETYMOLOGY.\\r',\n",
       " '\\r',\n",
       " 'EXTRACTS (Supplied by a Sub-Sub-Librarian).\\r',\n",
       " '\\r',\n",
       " 'CHAPTER 1. Loomings.\\r',\n",
       " '\\r']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizaremos un split en los datos, para poder clasificarlos\n",
    "# por parrafos, por lo que el split se realiza entorno a \"\\n\" que\n",
    "# dentro de los datos represetna un salto de linea entre parrafos\n",
    "\n",
    "\n",
    "list_libro = str_libro.split(\"\\n\")\n",
    "list_libro[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\r',\n",
       " '\\r',\n",
       " '\\r',\n",
       " '\\r',\n",
       " 'MOBY-DICK;\\r',\n",
       " '\\r',\n",
       " 'or, THE WHALE.\\r',\n",
       " '\\r',\n",
       " 'By Herman Melville\\r',\n",
       " '\\r',\n",
       " '\\r',\n",
       " '\\r',\n",
       " 'CONTENTS\\r',\n",
       " '\\r',\n",
       " 'ETYMOLOGY.\\r',\n",
       " '\\r',\n",
       " 'EXTRACTS (Supplied by a Sub-Sub-Librarian).\\r',\n",
       " '\\r',\n",
       " 'CHAPTER 1. Loomings.\\r',\n",
       " '\\r']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "libro_filitrado = list(filter(None, list_libro))\n",
    "libro_filitrado[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\juanj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\juanj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
